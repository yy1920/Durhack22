# -*- coding: utf-8 -*-
"""Durhack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13AVsF6xMzvYjgfuM9STU6nQhpdZyjQID
"""

# Mounting the drive
from google.colab import drive
drive.mount('/content/drive')

!pip3 install gsppy  
!pip install gsppy --upgrade --no-deps

!pip3 install apyori



# Importing important librairies
import pandas as pd
import numpy as np
from datetime import datetime
import pytz
import argparse
import logging
import random
from apyori import apriori  
logging.basicConfig(level=logging.DEBUG)
from gsppy.gsp import GSP
from urllib.parse import urlparse
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from xgboost import plot_importance
import xgboost
from matplotlib import pyplot
import pandas as pd
from sklearn import preprocessing
from numpy import sort
from numpy import loadtxt
from numpy import sort
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectFromModel

x_test = pd.read_csv('/content/drive/My Drive/Data_Science/durack_x_test.csv')
x_train = pd.read_csv('/content/drive/My Drive/Data_Science/durack_x_train.csv')
y_train = pd.read_csv('/content/drive/My Drive/Data_Science/durack_y_train.csv')

x_train

x_train = x_train.drop(columns=['ID','DATE'])

x_train = x_train.fillna(x_train.mean())

x_train.head()

x_train.iloc[:, :6]

no_scale = x_train.iloc[:, :6]
to_scale  = x_train.iloc[:, 6:]
temp = to_scale.columns

min_max_scaler = preprocessing.MinMaxScaler()
to_scale = min_max_scaler.fit_transform(to_scale)
to_scale = pd.DataFrame(to_scale)

to_scale.columns = temp
x_train = pd.concat([no_scale,to_scale], axis=1)



# fit model no training data
model = XGBClassifier()
model.fit(x_train, y_train['RET'])
# plot feature importance



# plot_importance(model)
# fig, ax  = plt.subplots(1, 1, figsize = (50, 30))

fig, ax = plt.subplots(figsize=(15, 15))
xgboost.plot_importance(model, ax=ax)

model.predict(x_train)

model.feature_importances_

thresholds = sort(model.feature_importances_)
for thresh in thresholds:
	# select features using threshold
	selection = SelectFromModel(model, threshold=thresh, prefit=True)
	select_X_train = selection.transform(x_train)
	# train model
	selection_model = XGBClassifier()
	selection_model.fit(select_X_train, y_train['RET'])
	# eval model
	# select_X_test = selection.transform(X_test)
	predictions = selection_model.predict(select_X_train)
	accuracy = accuracy_score(y_train['RET'], predictions)
	print("Thresh=%.3f, n=%d, Accuracy: %.2f%%" % (thresh, select_X_train.shape[1], accuracy*100.0))

